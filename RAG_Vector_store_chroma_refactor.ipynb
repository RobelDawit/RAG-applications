{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVjmkrsrM1IE74fJz5UuUQ",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Refactored RAG with Chroma (ready-to-run notebook)\n",
        "\n",
        "This notebook is a refactored, modular version of the original notebook:\n",
        "https://github.com/RobelDawit/RAG-applications/blob/a32a108d87327a56e778e76adcee8dfa1efaedcd/RAG_Vector_store_chroma.ipynb\n",
        "\n",
        "It provides reusable functions to:\n",
        "- set the OPENAI_API_KEY interactively,\n",
        "- load and split a PDF,\n",
        "- create OpenAI embeddings,\n",
        "- build and persist a Chroma vector store,\n",
        "- load the persisted store and run retrieval QA queries.\n",
        "\n",
        "Run the cells below in order. If you're using Google Colab, the optional drive mount cell will help persist the Chroma DB to your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "If running in Colab, run the cell below. If running locally, install the packages in your environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pip-install"
      },
      "source": [
        "!pip install -qU pinecone-client langchain openai pypdf unstructured langchain-community tiktoken langchain-openai chromadb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## Imports and logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imports-code"
      },
      "source": [
        "import os\n",
        "import textwrap\n",
        "import logging\n",
        "from typing import List, Optional\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helpers-md"
      },
      "source": [
        "## Helper functions\n",
        "\n",
        "These are the modular functions used to build/load the vector store and query it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "helpers-code"
      },
      "source": [
        "def set_env_from_prompt(var_name: str, force: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Prompt user to enter a value for an environment variable if it's not already set.\n",
        "    Useful in interactive runs (like Colab).\n",
        "    \"\"\"\n",
        "    if force or not os.environ.get(var_name):\n",
        "        try:\n",
        "            import getpass\n",
        "            value = getpass.getpass(f\"{var_name}: \")\n",
        "        except Exception:\n",
        "            value = input(f\"{var_name}: \")\n",
        "        os.environ[var_name] = value\n",
        "\n",
        "def load_pdf_documents(pdf_path: str) -> List:\n",
        "    \"\"\"\n",
        "    Load a PDF into LangChain Document objects using PyPDFLoader.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(pdf_path):\n",
        "        raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
        "    logger.info(\"Loading PDF: %s\", pdf_path)\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    docs = loader.load()\n",
        "    logger.info(\"Loaded %d raw documents/pages\", len(docs))\n",
        "    return docs\n",
        "\n",
        "def split_documents(documents: List, chunk_size: int = 500, chunk_overlap: int = 50) -> List:\n",
        "    \"\"\"\n",
        "    Split documents into smaller chunks for embedding.\n",
        "    \"\"\"\n",
        "    logger.info(\"Splitting documents with chunk_size=%d overlap=%d\", chunk_size, chunk_overlap)\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    split_docs = splitter.split_documents(documents)\n",
        "    logger.info(\"Split into %d chunks\", len(split_docs))\n",
        "    return split_docs\n",
        "\n",
        "def create_embeddings(openai_api_key: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    Create an embeddings object. If openai_api_key provided, set it into env.\n",
        "    \"\"\"\n",
        "    if openai_api_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "        raise EnvironmentError(\"OPENAI_API_KEY is not set. Call set_env_from_prompt or set the env var.\")\n",
        "    logger.info(\"Creating OpenAI embeddings client\")\n",
        "    return OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def build_chroma_vector_store(documents, embeddings, persist_directory: str):\n",
        "    \"\"\"\n",
        "    Build and persist a Chroma vector store from documents + embeddings.\n",
        "    \"\"\"\n",
        "    logger.info(\"Creating Chroma vector store at %s\", persist_directory)\n",
        "    vectordb = Chroma.from_documents(documents, embeddings, persist_directory=persist_directory)\n",
        "    logger.info(\"Persisted vector store to %s\", persist_directory)\n",
        "    return vectordb\n",
        "\n",
        "def load_chroma_vector_store(embeddings, persist_directory: str):\n",
        "    \"\"\"\n",
        "    Load an existing Chroma vector store (persisted).\n",
        "    \"\"\"\n",
        "    if not os.path.exists(persist_directory):\n",
        "        raise FileNotFoundError(f\"Persist directory not found: {persist_directory}\")\n",
        "    logger.info(\"Loading Chroma vector store from %s\", persist_directory)\n",
        "    return Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "def run_query_on_store(vector_store, query: str, k: int = 10) -> str:\n",
        "    \"\"\"\n",
        "    Run a RetrievalQA chain using OpenAI LLM and the provided vector store retriever.\n",
        "    Returns the answer string.\n",
        "    \"\"\"\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": k})\n",
        "    llm = OpenAI()  # customize model args here if desired\n",
        "    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "    logger.info(\"Running query against vector store (k=%d)\", k)\n",
        "    response = qa_chain.run(query)\n",
        "    return response\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-mount"
      },
      "source": [
        "## (Optional) Mount Google Drive (Colab)\n",
        "\n",
        "If you are running in Google Colab and want to persist the Chroma DB to Drive, run the cell below and set the paths accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drive-mount-code"
      },
      "source": [
        "# Uncomment and run in Colab if needed\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "#\n",
        "# # Example paths (adjust to your Drive layout)\n",
        "# pdf_path = '/content/drive/MyDrive/Books/Aircraft Structures By Megson ( PDFDrive ).pdf'\n",
        "# persist_dir = '/content/drive/MyDrive/chroma_db'\n",
        " \n",
        "# Or for local runs, set paths directly:\n",
        "pdf_path = '/path/to/your/Aircraft Structures By Megson ( PDFDrive ).pdf'  # <- change this\n",
        "persist_dir = './chroma_db'  # <- change this if you want a different local path\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "build-note"
      },
      "source": [
        "## Build / Create vector store\n",
        "\n",
        "Run the cell below to build and persist the Chroma vector store. This may take some time depending on the PDF size and network latency to the embeddings API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "build-code"
      },
      "source": [
        "# Ensure OPENAI_API_KEY exists; will prompt if not set\n",
        "try:\n",
        "    if not os.environ.get('OPENAI_API_KEY'):\n",
        "        set_env_from_prompt('OPENAI_API_KEY')\n",
        "    emb = create_embeddings()\n",
        "    # Load and split\n",
        "    docs = load_pdf_documents(pdf_path)\n",
        "    split_docs = split_documents(docs, chunk_size=500, chunk_overlap=50)\n",
        "    # Build and persist\n",
        "    vectordb = build_chroma_vector_store(split_docs, emb, persist_directory=persist_dir)\n",
        "    logger.info('Vector store built and persisted at %s', persist_dir)\n",
        "except Exception as e:\n",
        "    logger.exception('Error while building vector store: %s', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "query-note"
      },
      "source": [
        "## Query the persisted vector store\n",
        "\n",
        "Run this cell to load the persisted Chroma DB and ask a question. Adjust the query and k (number of retrieved chunks) as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "query-code"
      },
      "source": [
        "try:\n",
        "    # Ensure embeddings client is available (it uses OPENAI_API_KEY)\n",
        "    emb = create_embeddings()\n",
        "    vectordb = load_chroma_vector_store(emb, persist_directory=persist_dir)\n",
        "    query = \"Give me a high-level overview of the most important structures (spars, stringers, frames, bulkheads) on a loaded aircraft in flight and a brief overview of what they do.\"\n",
        "    answer = run_query_on_store(vectordb, query, k=10)\n",
        "    print('\\nAnswer:\\n')\n",
        "    print(textwrap.fill(answer, width=100))\n",
        "except Exception as e:\n",
        "    logger.exception('Error while querying vector store: %s', e)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final-notes"
      },
      "source": [
        "## Notes & next steps\n",
        "- If you prefer to run from the command line, I also provided a CLI-style script in the earlier refactor (you can copy the same functions into a .py file).\n",
        "- For large PDFs consider batching embeddings, adding progress reporting, or using a remote vector DB (Pinecone, Weaviate) for scale.\n",
        "- If your LangChain or provider package names differ (versions change often), update the imports accordingly (e.g., `langchain.embeddings` vs `langchain_openai`).\n",
        "\n",
        "If you'd like, I can also:\n",
        "- add batching for embeddings,\n",
        "- split by headings using a different text splitter,\n",
        "- or convert this notebook into a runnable GitHub Actions workflow to index PDFs automatically."
      ]
    }
  ]
}